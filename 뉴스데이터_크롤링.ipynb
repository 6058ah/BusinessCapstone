{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "뉴스데이터 크롤링.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPmmQVDu0s9GNwXT3r6gxGz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/6058ah/BusinessCapstone/blob/master/%EB%89%B4%EC%8A%A4%EB%8D%B0%EC%9D%B4%ED%84%B0_%ED%81%AC%EB%A1%A4%EB%A7%81.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fac-_weF6Hc9",
        "outputId": "43b7b32e-26f2-46c4-df02-0fd433888ed5"
      },
      "source": [
        "!pip install xlsxwriter"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: xlsxwriter in /usr/local/lib/python3.7/dist-packages (1.4.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gv09vQzKsnF6",
        "outputId": "65c2e02c-337f-419a-d356-2ec51ecd52b4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87RqPsLxkriB"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime\n",
        "import requests\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "'''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
        "< naver 뉴스 검색시 리스트 크롤링하는 프로그램 > _select사용\n",
        "- 크롤링 해오는 것 : 링크,제목,신문사,내용요약본\n",
        "- 내용요약본  -> 정제 작업 필요\n",
        "- 리스트 -> 딕셔너리 -> df -> 엑셀로 저장\n",
        "'''''''''''''''''''''\n",
        "\n",
        "title_text=[]\n",
        "contents_text=[]\n",
        "dates_text = []\n",
        "#result=[]\n",
        "\n",
        "#엑셀로 저장하기 위한 변수\n",
        "RESULT_PATH ='/content/drive'  #결과 저장할 경로\n",
        "\n",
        "#날짜 범위 리스트로 만들기\n",
        "def date_range(start, end):\n",
        "    start = datetime.strptime(start, \"%Y.%m.%d\")\n",
        "    end = datetime.strptime(end, \"%Y.%m.%d\")\n",
        "    dates = [date.strftime(\"%Y.%m.%d\") for date in pandas.date_range(start, periods=(end-start).days+1)]\n",
        "    return dates\n",
        "\n",
        "#def contents_cleansing(contents):\n",
        "#    first_cleansing_contents = re.sub('<dl>.*?</a> </div> </dd> <dd>', '',str(contents)).strip()  #앞에 필요없는 부분 제거\n",
        "#    second_cleansing_contents = re.sub('<ul class=\"relation_lst\">.*?</dd>', '', first_cleansing_contents).strip()#뒤에 필요없는 부분 제거 (새끼 기사)\n",
        "#    third_cleansing_contents = re.sub('<.+?>', '', second_cleansing_contents).strip()\n",
        "#    contents_text.append(third_cleansing_contents)\n",
        "#    #print(contents_text)\n",
        "\n",
        "#크롤링 시작\n",
        "def crawler(query,s_date,e_date):\n",
        "  dates= date_range(s_date,e_date)\n",
        "  page_num = [1,11,21,31,41]\n",
        "  #maxpage_t =41\n",
        "  for x in dates:\n",
        "    #print(x)\n",
        "    for page_number in page_num:\n",
        "      ds = x.replace(\".\",\"\")\n",
        "      de = ds\n",
        "      url = \"https://search.naver.com/search.naver?where=news&query=\" + query + \"&sort=0\"+\"&ds=\" + x + \"&de=\" + x + \"&nso=so%3Ar%2Cp%3Afrom\" + ds + \"to\" + de + \"%2Ca%3A&start=\" + str(page_number)\n",
        "      #url = \"https://search.naver.com/search.naver?&where=news&query={0}&sm=tab_pge&sort={1}&photo=0&field=0&reporter_article=&pd=3&ds={2}&de={2}&docid=&nso=so:r,p:,a:all&mynews=1&cluster_rank=238&start={3}&refresh_start=0\".format(query,sort,x,str(page_num))\n",
        "      #print(url)\n",
        "      response = requests.get(url)\n",
        "      html = response.text\n",
        "\n",
        "      #뷰티풀소프의 인자값 지정\n",
        "      soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "      #뉴스 날짜\n",
        "#      bbs = soup.find_all('span', 'info')\n",
        "#      for bb in bbs:\n",
        "#        t = bb.text\n",
        "#        dates_text.append(t)\n",
        "#      search = \"면\"\n",
        "#      for word in dates_text:\n",
        "#        if search in word: \n",
        "#          dates_text.remove(word)\n",
        "\n",
        "\n",
        "      #<a>태그에서 제목과 링크주소 (a 태그 중 class 명이 news_tit인 것)\n",
        "      atags = soup.find_all('a', 'news_tit')\n",
        "      for atag in atags:\n",
        "        title = atag.get('title')\n",
        "        title_text.append(title)     #제목\n",
        "        dates_text.append(x)\n",
        "            \n",
        "          #본문요약본 (a 태그 중 class 명이 api_txt_lines dsc_txt_wrap인 것)\n",
        "      #contents_lists = soup.find_all('a','api_txt_lines dsc_txt_wrap')\n",
        "      #for contents_list in contents_lists:\n",
        "      #  contents_cleansing(contents_list) #본문요약 정제화\n",
        "\n",
        "\n",
        "      #모든 리스트 딕셔너리형태로 저장\n",
        "      #result= {\"Date\":dates_text,\"title\":title_text}\n",
        "      df1 = pd.DataFrame(dates_text)\n",
        "      df2 = pd.DataFrame(title_text) #df로 변환\n",
        "      result = pd.concat([df1,df2],axis=1)\n",
        "      #print(result)\n",
        "\n",
        "  outputFileName = '{0}~{1} {2} 관련 뉴스.xlsx'.format(s_date.replace(\".\",\"\"), e_date.replace(\".\",\"\"), query)\n",
        "  writer = pd.ExcelWriter(outputFileName, engine='xlsxwriter')\n",
        "  result.to_excel(writer, sheet_name= '{0}~{1} {2} 뉴스'.format(s_date.replace(\".\",\"\"), e_date.replace(\".\",\"\"), query))\n",
        "  writer.save()\n",
        "\n",
        "#메인함수\n",
        "def main():\n",
        "    info_main = input(\"=\"*50+\"\\n\"+\"입력 형식에 맞게 입력해주세요.\"+\"\\n\"+\" 시작하시려면 Enter를 눌러주세요.\"+\"\\n\"+\"=\"*50)\n",
        "    query = input(\"검색어 입력: \") #네이버, 부동산...\n",
        "    s_date = input(\"시작날짜 입력(예시:2019.01.01):\")\n",
        "    e_date = input(\"끝날짜 입력:\")\n",
        "    crawler(query,s_date,e_date)\n",
        "  "
      ],
      "execution_count": 232,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDeMius8cLgx"
      },
      "source": [
        "main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fDf-dOW855Q"
      },
      "source": [
        "#참고\n",
        "##https://hansuho113.tistory.com/3?category=913503\n",
        "##https://adsp-ggini.tistory.com/80"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rETGrAWcCeh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}